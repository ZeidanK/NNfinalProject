{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830fe29e",
   "metadata": {},
   "source": [
    "# Notebook 05: creditcard Neural Network Training & Regularization\n",
    "\n",
    "**Dataset**: creditcard.csv (Real-World ULB)\n",
    "\n",
    "**Objective**: Apply best architecture from card_transdata experiments and test 8 regularization strategies (REG-01 to REG-08) to optimize NN performance on production data.\n",
    "\n",
    "**Primary Metric**: PR-AUC (optimal for 0.17% fraud rate)\n",
    "\n",
    "**Goal**: Beat or match Random Forest baseline from Notebook 04 using neural networks.\n",
    "\n",
    "**Experiments**:\n",
    "1. Transfer best architecture from Notebook 03 (card_transdata)\n",
    "2. Test 8 regularization configurations (REG-01 to REG-08)\n",
    "3. Select best NN model for final test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a099b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import config\n",
    "from src.nn_architectures import build_fraud_detection_nn\n",
    "from src.nn_training_utils import train_nn_with_early_stopping, log_experiment\n",
    "from src.evaluation_metrics import compute_fraud_metrics, print_classification_summary\n",
    "from src.visualization_utils import (\n",
    "    plot_training_history,\n",
    "    plot_confusion_matrix,\n",
    "    plot_precision_recall_curve\n",
    ")\n",
    "\n",
    "# Set random seeds\n",
    "config.set_random_seeds()\n",
    "\n",
    "# Get dataset config\n",
    "ds_config = config.get_dataset_config('creditcard')\n",
    "\n",
    "print(\"âœ“ Imports complete\")\n",
    "print(f\"âœ“ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"âœ“ Random seed set to {config.RANDOM_SEED}\")\n",
    "print(f\"âœ“ Dataset: creditcard.csv (Real-World ULB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8ce597",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c68284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original data\n",
    "df = pd.read_csv(ds_config['data_path'])\n",
    "X = df[ds_config['feature_cols']].values\n",
    "y = df[ds_config['target_col']].values\n",
    "\n",
    "# Load saved split indices\n",
    "train_idx = np.load(ds_config['train_idx'])\n",
    "val_idx = np.load(ds_config['val_idx'])\n",
    "test_idx = np.load(ds_config['test_idx'])\n",
    "\n",
    "# Split data using saved indices\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_val, y_val = X[val_idx], y[val_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "# Load fitted scaler\n",
    "scaler = joblib.load(ds_config['scaler_path'])\n",
    "\n",
    "# Transform data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ“ Data loaded and preprocessed:\")\n",
    "print(f\"  Train: {X_train_scaled.shape[0]:,} samples\")\n",
    "print(f\"  Val:   {X_val_scaled.shape[0]:,} samples\")\n",
    "print(f\"  Test:  {X_test_scaled.shape[0]:,} samples (RESERVED for Notebook 06)\")\n",
    "print(f\"  Features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"\\nâœ“ Class distribution:\")\n",
    "print(f\"  Train fraud rate: {y_train.mean()*100:.4f}%\")\n",
    "print(f\"  Val fraud rate:   {y_val.mean()*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31baf4c2",
   "metadata": {},
   "source": [
    "## 2. Load Baseline Performance Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d2e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline results from Notebook 04\n",
    "baseline_results = pd.read_csv(ds_config['tables_dir'] / 'baseline_results.csv')\n",
    "\n",
    "print(\"ðŸ“Š Baseline Performance Targets (Validation Set):\")\n",
    "print(\"=\"*60)\n",
    "print(baseline_results.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract RF performance as target\n",
    "rf_pr_auc = baseline_results[baseline_results['model'] == 'Random Forest']['pr_auc'].values[0]\n",
    "rf_f1 = baseline_results[baseline_results['model'] == 'Random Forest']['f1_fraud'].values[0]\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Target to Beat/Match:\")\n",
    "print(f\"   Random Forest PR-AUC: {rf_pr_auc:.4f}\")\n",
    "print(f\"   Random Forest F1:     {rf_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af48e6d5",
   "metadata": {},
   "source": [
    "## 3. Load Best Architecture from card_transdata Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load architecture ranking from Notebook 03\n",
    "card_transdata_results = config.RESULTS_DIR / 'card_transdata' / 'tables' / 'architecture_ranking.csv'\n",
    "\n",
    "if card_transdata_results.exists():\n",
    "    arch_ranking = pd.read_csv(card_transdata_results)\n",
    "    best_arch = arch_ranking.iloc[0]\n",
    "    \n",
    "    print(\"âœ“ Loaded best architecture from card_transdata experiments:\")\n",
    "    print(f\"  Experiment ID: {best_arch['experiment_id']}\")\n",
    "    print(f\"  Architecture: {best_arch['architecture']}\")\n",
    "    print(f\"  Layers: {best_arch['layers']}\")\n",
    "    print(f\"  card_transdata PR-AUC: {best_arch['pr_auc']:.4f}\")\n",
    "    \n",
    "    # Parse layers string back to list\n",
    "    import ast\n",
    "    best_arch_layers = ast.literal_eval(best_arch['layers'])\n",
    "    print(f\"\\nðŸš€ Transferring architecture to creditcard dataset...\")\n",
    "else:\n",
    "    print(\"âš ï¸  Warning: No architecture ranking found from Notebook 03\")\n",
    "    print(\"   Using default architecture: [64, 32]\")\n",
    "    best_arch_layers = [64, 32]\n",
    "    best_arch = {'experiment_id': 'DEFAULT', 'architecture': 'balanced_medium'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b2ad52",
   "metadata": {},
   "source": [
    "## 4. Regularization Experiments (REG-01 to REG-08)\n",
    "\n",
    "Test 8 regularization strategies on real-world data using transferred architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180842cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experiment log\n",
    "regularization_results = []\n",
    "\n",
    "# Create results directory\n",
    "(ds_config['models_dir'] / 'neural_networks').mkdir(parents=True, exist_ok=True)\n",
    "(ds_config['experiment_logs_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" REGULARIZATION EXPERIMENTS: Testing 8 Strategies\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Architecture: {best_arch_layers}\")\n",
    "print(f\"Dataset: creditcard.csv (Real-World ULB)\")\n",
    "print(f\"Target: RF PR-AUC = {rf_pr_auc:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38773b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through regularization configurations\n",
    "for reg_id, reg_config in config.REGULARIZATION_EXPERIMENTS.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {reg_id}: {reg_config['description']}\")\n",
    "    print(f\"Dropout={reg_config['dropout']}, L2={reg_config['l2']}, BatchNorm={reg_config['batch_norm']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_fraud_detection_nn(\n",
    "        input_dim=X_train_scaled.shape[1],\n",
    "        hidden_layers=best_arch_layers,\n",
    "        dropout_rate=reg_config['dropout'],\n",
    "        l2_reg=reg_config['l2'],\n",
    "        use_batch_norm=reg_config['batch_norm']\n",
    "    )\n",
    "    \n",
    "    # Train with early stopping\n",
    "    history = train_nn_with_early_stopping(\n",
    "        model=model,\n",
    "        X_train=X_train_scaled,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val_scaled,\n",
    "        y_val=y_val,\n",
    "        epochs=config.MAX_EPOCHS,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        patience=config.EARLY_STOPPING_PATIENCE,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    y_val_pred_proba = model.predict(X_val_scaled, verbose=0).flatten()\n",
    "    y_val_pred = (y_val_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = compute_fraud_metrics(y_val, y_val_pred, y_val_pred_proba)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š {reg_id} Validation Performance:\")\n",
    "    print(f\"  PR-AUC:        {metrics['pr_auc']:.4f} (Target: {rf_pr_auc:.4f})\")\n",
    "    print(f\"  ROC-AUC:       {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"  F1 (Fraud):    {metrics['f1_fraud']:.4f} (Target: {rf_f1:.4f})\")\n",
    "    print(f\"  Recall (Fraud): {metrics['recall_fraud']:.4f}\")\n",
    "    print(f\"  Precision (Fraud): {metrics['precision_fraud']:.4f}\")\n",
    "    \n",
    "    # Compare to RF\n",
    "    pr_auc_diff = metrics['pr_auc'] - rf_pr_auc\n",
    "    if pr_auc_diff >= 0:\n",
    "        print(f\"  ðŸŽ‰ BEATS RF by {pr_auc_diff*100:.2f} percentage points!\")\n",
    "    else:\n",
    "        print(f\"  ðŸ“‰ Below RF by {abs(pr_auc_diff)*100:.2f} percentage points\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = ds_config['models_dir'] / 'neural_networks' / f'{reg_id}_regularization.keras'\n",
    "    model.save(model_path)\n",
    "    print(f\"\\nâœ“ Model saved to: {model_path}\")\n",
    "    \n",
    "    # Plot learning curves\n",
    "    fig_path = ds_config['figures_dir'] / 'learning_curves' / f'{reg_id}_learning_curves.png'\n",
    "    plot_training_history(\n",
    "        history,\n",
    "        save_path=str(fig_path),\n",
    "        title=f'{reg_id}: {reg_config[\"description\"]} - Learning Curves'\n",
    "    )\n",
    "    \n",
    "    # Log experiment\n",
    "    log_experiment(\n",
    "        experiment_id=reg_id,\n",
    "        dataset_name='creditcard',\n",
    "        experiment_type='regularization',\n",
    "        model_name=f\"{best_arch['architecture']}_reg\",\n",
    "        architecture=best_arch_layers,\n",
    "        hyperparameters={\n",
    "            'dropout': reg_config['dropout'],\n",
    "            'l2': reg_config['l2'],\n",
    "            'batch_norm': reg_config['batch_norm'],\n",
    "            'batch_size': config.BATCH_SIZE,\n",
    "            'epochs_trained': len(history.history['loss'])\n",
    "        },\n",
    "        metrics=metrics,\n",
    "        log_path=ds_config['experiment_logs_dir'] / 'nn_experiments.csv'\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    regularization_results.append({\n",
    "        'experiment_id': reg_id,\n",
    "        'description': reg_config['description'],\n",
    "        'dropout': reg_config['dropout'],\n",
    "        'l2': reg_config['l2'],\n",
    "        'batch_norm': reg_config['batch_norm'],\n",
    "        'pr_auc': metrics['pr_auc'],\n",
    "        'roc_auc': metrics['roc_auc'],\n",
    "        'f1_fraud': metrics['f1_fraud'],\n",
    "        'recall_fraud': metrics['recall_fraud'],\n",
    "        'precision_fraud': metrics['precision_fraud'],\n",
    "        'vs_rf': pr_auc_diff\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ“ Regularization Experiments Complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091d855b",
   "metadata": {},
   "source": [
    "## 5. Regularization Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30756ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "reg_df = pd.DataFrame(regularization_results)\n",
    "reg_df = reg_df.sort_values('pr_auc', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\" REGULARIZATION RANKING BY PR-AUC (Validation Set)\")\n",
    "print(\"=\"*90)\n",
    "print(reg_df[['experiment_id', 'description', 'pr_auc', 'f1_fraud', 'vs_rf']].to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Identify best regularization\n",
    "best_reg = reg_df.iloc[0]\n",
    "print(f\"\\nðŸ† Best Regularization: {best_reg['experiment_id']}\")\n",
    "print(f\"   Description: {best_reg['description']}\")\n",
    "print(f\"   Dropout: {best_reg['dropout']}, L2: {best_reg['l2']}, BatchNorm: {best_reg['batch_norm']}\")\n",
    "print(f\"   PR-AUC: {best_reg['pr_auc']:.4f}\")\n",
    "print(f\"   F1 (Fraud): {best_reg['f1_fraud']:.4f}\")\n",
    "print(f\"   vs RF: {best_reg['vs_rf']*100:+.2f} percentage points\")\n",
    "\n",
    "# Save ranking\n",
    "reg_ranking_path = ds_config['tables_dir'] / 'regularization_ranking.csv'\n",
    "reg_df.to_csv(reg_ranking_path, index=False)\n",
    "print(f\"\\nâœ“ Regularization ranking saved to: {reg_ranking_path}\")\n",
    "\n",
    "# Count models that beat RF\n",
    "models_beat_rf = (reg_df['vs_rf'] > 0).sum()\n",
    "print(f\"\\nðŸ“Š Summary: {models_beat_rf}/{len(reg_df)} models beat Random Forest baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a982f5c",
   "metadata": {},
   "source": [
    "## 6. Visualize Best Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52fc687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model_path = ds_config['models_dir'] / 'neural_networks' / f'{best_reg[\"experiment_id\"]}_regularization.keras'\n",
    "best_model = keras.models.load_model(best_model_path)\n",
    "\n",
    "# Generate predictions\n",
    "y_val_pred_proba = best_model.predict(X_val_scaled, verbose=0).flatten()\n",
    "y_val_pred = (y_val_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    y_val, y_val_pred, \n",
    "    ['Legitimate', 'Fraud'],\n",
    "    save_path=str(ds_config['figures_dir'] / 'best_nn_confusion_matrix.png'),\n",
    "    title=f'Best NN ({best_reg[\"experiment_id\"]}) - Confusion Matrix'\n",
    ")\n",
    "\n",
    "# PR curve\n",
    "plot_precision_recall_curve(\n",
    "    y_val, y_val_pred_proba,\n",
    "    save_path=str(ds_config['figures_dir'] / 'best_nn_pr_curve.png'),\n",
    "    title=f'Best NN ({best_reg[\"experiment_id\"]}) - PR Curve'\n",
    ")\n",
    "\n",
    "print(\"âœ“ Best model visualizations saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b7020",
   "metadata": {},
   "source": [
    "## 7. Save Best Model for Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77349884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy best model to designated location\n",
    "import shutil\n",
    "\n",
    "best_model_final_path = ds_config['models_dir'] / 'best_nn_models' / 'best_nn_for_test.keras'\n",
    "best_model_final_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "shutil.copy(best_model_path, best_model_final_path)\n",
    "\n",
    "# Save metadata\n",
    "best_model_metadata = {\n",
    "    'experiment_id': best_reg['experiment_id'],\n",
    "    'description': best_reg['description'],\n",
    "    'architecture': best_arch_layers,\n",
    "    'dropout': float(best_reg['dropout']),\n",
    "    'l2': float(best_reg['l2']),\n",
    "    'batch_norm': bool(best_reg['batch_norm']),\n",
    "    'val_pr_auc': float(best_reg['pr_auc']),\n",
    "    'val_f1_fraud': float(best_reg['f1_fraud']),\n",
    "    'vs_rf_baseline': float(best_reg['vs_rf'])\n",
    "}\n",
    "\n",
    "metadata_path = ds_config['models_dir'] / 'best_nn_models' / 'best_model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(best_model_metadata, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Best model saved for test evaluation:\")\n",
    "print(f\"  Model: {best_model_final_path}\")\n",
    "print(f\"  Metadata: {metadata_path}\")\n",
    "print(f\"\\nâš ï¸  TEST SET RESERVED: Do not evaluate until Notebook 06 (threshold optimization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc3d199",
   "metadata": {},
   "source": [
    "## 8. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" NOTEBOOK 05 SUMMARY - creditcard NN TRAINING & REGULARIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ† Best Neural Network Model: {best_reg['experiment_id']}\")\n",
    "print(f\"   Description: {best_reg['description']}\")\n",
    "print(f\"   Architecture: {best_arch_layers}\")\n",
    "print(f\"   Regularization: Dropout={best_reg['dropout']}, L2={best_reg['l2']}, BatchNorm={best_reg['batch_norm']}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Validation Performance:\")\n",
    "print(f\"   PR-AUC:     {best_reg['pr_auc']:.4f}\")\n",
    "print(f\"   F1 (Fraud): {best_reg['f1_fraud']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ vs Random Forest Baseline:\")\n",
    "print(f\"   RF PR-AUC:  {rf_pr_auc:.4f}\")\n",
    "print(f\"   NN PR-AUC:  {best_reg['pr_auc']:.4f}\")\n",
    "print(f\"   Difference: {best_reg['vs_rf']*100:+.2f} percentage points\")\n",
    "\n",
    "if best_reg['vs_rf'] > 0:\n",
    "    print(\"\\nâœ… SUCCESS: Neural network BEATS Random Forest on real-world data!\")\n",
    "elif abs(best_reg['vs_rf']) < 0.01:\n",
    "    print(\"\\nâœ… SUCCESS: Neural network MATCHES Random Forest performance\")\n",
    "else:\n",
    "    print(\"\\nðŸ“Š Neural network below RF, but provides:\")\n",
    "    print(\"   - Probability calibration for threshold optimization\")\n",
    "    print(\"   - Interpretable regularization insights\")\n",
    "    print(\"   - Production deployment flexibility\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Experiments Summary:\")\n",
    "print(f\"   Models trained: {len(reg_df)}\")\n",
    "print(f\"   Models beat RF: {models_beat_rf}\")\n",
    "print(f\"   Best PR-AUC: {reg_df['pr_auc'].max():.4f}\")\n",
    "print(f\"   Worst PR-AUC: {reg_df['pr_auc'].min():.4f}\")\n",
    "\n",
    "print(f\"\\nðŸš€ Next Steps (Notebook 06):\")\n",
    "print(f\"   1. Optimize classification threshold on validation set\")\n",
    "print(f\"   2. ONE-TIME test set evaluation (final performance)\")\n",
    "print(f\"   3. Compare NN vs RF on test set\")\n",
    "print(f\"   4. Generate final performance report\")\n",
    "\n",
    "print(\"\\nðŸ“ Artifacts Created:\")\n",
    "print(f\"   - {len(regularization_results)} regularization models\")\n",
    "print(f\"   - Best model: {best_model_final_path}\")\n",
    "print(f\"   - Experiment logs: {ds_config['experiment_logs_dir']}/nn_experiments.csv\")\n",
    "print(f\"   - Learning curves: {ds_config['figures_dir']}/learning_curves/\")\n",
    "print(f\"   - Rankings: {reg_ranking_path}\")\n",
    "\n",
    "print(\"\\nâœ… Notebook 05 Complete!\")\n",
    "print(\"ðŸŽ¯ Ready for Notebook 06: Threshold Optimization & Test Evaluation\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
