{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81873a2",
   "metadata": {},
   "source": [
    "# Notebook 03: Neural Network Architecture Exploration\n",
    "\n",
    "**Objective**: Systematically compare MLP architectures of varying depth and width to identify the optimal configuration for fraud detection.\n",
    "\n",
    "**Research Questions**:\n",
    "1. How does network depth affect fraud detection performance?\n",
    "2. Does width compensate for lack of depth?\n",
    "3. What is the optimal architecture-performance-complexity tradeoff?\n",
    "\n",
    "**Architecture Categories**:\n",
    "- **Shallow**: [32], [64, 32]\n",
    "- **Medium**: [128, 64, 32], [256, 128, 64, 32]\n",
    "- **Deep**: [512, 256, 128, 64, 32, 16]\n",
    "- **Wide**: [256], [512]\n",
    "\n",
    "**Training Configuration**: All models trained with identical hyperparameters (epochs=100, batch_size=64, early_stopping patience=15) to ensure fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb57a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import config\n",
    "from src.nn_architectures import get_architecture_by_name, count_parameters\n",
    "from src.nn_training_utils import train_neural_network, get_class_weights\n",
    "from src.evaluation_metrics import compute_fraud_metrics, print_classification_summary\n",
    "from src.visualization_utils import plot_learning_curves, plot_precision_recall_curve, plot_architecture_comparison\n",
    "\n",
    "# Set random seeds\n",
    "config.set_random_seeds()\n",
    "\n",
    "print(\"âœ“ Imports complete\")\n",
    "print(f\"âœ“ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"âœ“ Random seed set to {config.RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18421d9",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data\n",
    "\n",
    "**Critical**: We load the **saved splits and scaler** from Notebook 02 to ensure reproducibility and prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a1a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original data\n",
    "df = pd.read_csv(config.DATASET_PATH)\n",
    "X = df[config.FEATURE_COLUMNS].values\n",
    "y = df[config.TARGET_COLUMN].values\n",
    "\n",
    "# Load saved split indices\n",
    "train_idx = np.load(config.TRAIN_INDICES_PATH)\n",
    "val_idx = np.load(config.VAL_INDICES_PATH)\n",
    "test_idx = np.load(config.TEST_INDICES_PATH)\n",
    "\n",
    "# Split data using saved indices\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_val, y_val = X[val_idx], y[val_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "# Load fitted scaler and transform data\n",
    "scaler = joblib.load(config.FITTED_SCALER_PATH)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ“ Data loaded and split using saved indices\")\n",
    "print(f\"  Train: {X_train_scaled.shape[0]:,} samples\")\n",
    "print(f\"  Val:   {X_val_scaled.shape[0]:,} samples\")\n",
    "print(f\"  Test:  {X_test_scaled.shape[0]:,} samples\")\n",
    "print(f\"\\nâœ“ Scaler loaded from: {config.FITTED_SCALER_PATH}\")\n",
    "print(f\"âœ“ Input dimension: {X_train_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51ec6a",
   "metadata": {},
   "source": [
    "## 2. Compute Class Weights\n",
    "\n",
    "Class weights handle the severe imbalance (124:1) by penalizing misclassifications of the minority class more heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef284b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute balanced class weights\n",
    "class_weights = get_class_weights(y_train)\n",
    "\n",
    "print(\"âœ“ Class weights computed:\")\n",
    "print(f\"  Class 0 (Legitimate): {class_weights[0]:.4f}\")\n",
    "print(f\"  Class 1 (Fraud):      {class_weights[1]:.4f}\")\n",
    "print(f\"  Ratio: {class_weights[1]/class_weights[0]:.1f}x penalty for fraud misclassification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06a8af",
   "metadata": {},
   "source": [
    "## 3. Define Architecture Experiment Plan\n",
    "\n",
    "We'll train 8 different architectures spanning shallow to deep and narrow to wide configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c42f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define architectures to test\n",
    "architectures_to_test = [\n",
    "    # Shallow networks\n",
    "    (\"shallow_tiny\", [32]),\n",
    "    (\"shallow_small\", [64, 32]),\n",
    "    \n",
    "    # Medium networks\n",
    "    (\"medium_base\", [128, 64, 32]),\n",
    "    (\"medium_deep\", [256, 128, 64, 32]),\n",
    "    \n",
    "    # Deep network\n",
    "    (\"deep\", [512, 256, 128, 64, 32, 16]),\n",
    "    \n",
    "    # Wide networks\n",
    "    (\"wide_medium\", [256]),\n",
    "    (\"wide_large\", [512]),\n",
    "    \n",
    "    # Balanced (for later comparison)\n",
    "    (\"balanced\", [256, 128, 64])\n",
    "]\n",
    "\n",
    "print(\"âœ“ Architecture Experiment Plan:\")\n",
    "print(f\"  Total architectures: {len(architectures_to_test)}\")\n",
    "print(\"\\n  Architectures:\")\n",
    "for name, layers in architectures_to_test:\n",
    "    params = count_parameters(len(config.FEATURE_COLUMNS), layers)\n",
    "    print(f\"    {name:20} {str(layers):30} â†’ {params:,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9fc74",
   "metadata": {},
   "source": [
    "## 4. Train All Architectures\n",
    "\n",
    "We train each architecture with identical hyperparameters to ensure fair comparison. Early stopping prevents overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d298ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for results\n",
    "results = []\n",
    "histories = {}\n",
    "\n",
    "# Training configuration (consistent across all models)\n",
    "training_config = {\n",
    "    'epochs': 100,\n",
    "    'batch_size': 64,\n",
    "    'patience': 15,\n",
    "    'learning_rate': 0.001,\n",
    "    'dropout_rate': 0.0,  # No dropout in base comparison\n",
    "    'l2_reg': 0.0,        # No L2 in base comparison\n",
    "    'use_batch_norm': False\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING ARCHITECTURE EXPERIMENTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Configuration: {training_config}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Train each architecture\n",
    "for arch_name, hidden_layers in architectures_to_test:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training: {arch_name.upper()}\")\n",
    "    print(f\"Architecture: {hidden_layers}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create model\n",
    "    model = get_architecture_by_name(arch_name, len(config.FEATURE_COLUMNS))\n",
    "    \n",
    "    # Train model\n",
    "    history, trained_model, train_time_seconds = train_neural_network(\n",
    "        model=model,\n",
    "        X_train=X_train_scaled,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val_scaled,\n",
    "        y_val=y_val,\n",
    "        class_weights=class_weights,\n",
    "        epochs=training_config['epochs'],\n",
    "        batch_size=training_config['batch_size'],\n",
    "        patience=training_config['patience'],\n",
    "        experiment_name=f\"arch_{arch_name}\"\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    y_val_pred_proba = trained_model.predict(X_val_scaled, verbose=0).flatten()\n",
    "    y_val_pred = (y_val_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    metrics = compute_fraud_metrics(y_val, y_val_pred, y_val_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'architecture': arch_name,\n",
    "        'layers': str(hidden_layers),\n",
    "        'num_params': count_parameters(len(config.FEATURE_COLUMNS), hidden_layers),\n",
    "        'depth': len(hidden_layers),\n",
    "        'width': max(hidden_layers),\n",
    "        'train_time': train_time_seconds,\n",
    "        'final_epoch': len(history.history['loss']),\n",
    "        **metrics\n",
    "    })\n",
    "    \n",
    "    histories[arch_name] = history\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nâœ“ Training complete in {train_time_seconds:.2f}s ({len(history.history['loss'])} epochs)\")\n",
    "    print(f\"  PR-AUC: {metrics['pr_auc']:.4f} | ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"  F1 (Fraud): {metrics['fraud_f1']:.4f} | Recall: {metrics['fraud_recall']:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(config.MODELS_DIR, f'arch_{arch_name}.keras')\n",
    "    trained_model.save(model_path)\n",
    "    print(f\"  Model saved: {model_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ“ All architectures trained!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c84a8",
   "metadata": {},
   "source": [
    "## 5. Compare Architecture Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('pr_auc', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ARCHITECTURE COMPARISON - Sorted by PR-AUC\")\n",
    "print(\"=\"*100)\n",
    "print(results_df[['architecture', 'layers', 'num_params', 'pr_auc', 'roc_auc', \n",
    "                   'fraud_f1', 'fraud_recall', 'fraud_precision', 'train_time']].to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save results\n",
    "results_path = os.path.join(config.RESULTS_DIR, 'tables', 'architecture_comparison.csv')\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"\\nâœ“ Results saved to: {results_path}\")\n",
    "\n",
    "# Find best architecture\n",
    "best_arch = results_df.iloc[0]\n",
    "print(f\"\\nğŸ† Best Architecture: {best_arch['architecture']}\")\n",
    "print(f\"   Layers: {best_arch['layers']}\")\n",
    "print(f\"   PR-AUC: {best_arch['pr_auc']:.4f}\")\n",
    "print(f\"   Parameters: {best_arch['num_params']:,}\")\n",
    "print(f\"   Training time: {best_arch['train_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75615d1b",
   "metadata": {},
   "source": [
    "## 6. Visualize Architecture Tradeoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57ed670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive architecture comparison plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. Performance vs Complexity\n",
    "ax1 = axes[0, 0]\n",
    "scatter = ax1.scatter(results_df['num_params'], results_df['pr_auc'], \n",
    "                     c=results_df['depth'], cmap='viridis', s=200, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "for idx, row in results_df.iterrows():\n",
    "    ax1.annotate(row['architecture'], (row['num_params'], row['pr_auc']), \n",
    "                fontsize=8, ha='right', va='bottom')\n",
    "ax1.set_xlabel('Number of Parameters', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('PR-AUC (Validation)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Performance vs Model Complexity', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(scatter, ax=ax1)\n",
    "cbar.set_label('Network Depth', fontsize=10)\n",
    "\n",
    "# 2. Depth vs Performance\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(results_df['depth'], results_df['pr_auc'], s=200, alpha=0.7, color='steelblue', edgecolor='black', linewidth=2)\n",
    "for idx, row in results_df.iterrows():\n",
    "    ax2.annotate(row['architecture'], (row['depth'], row['pr_auc']), \n",
    "                fontsize=8, ha='center', va='bottom')\n",
    "ax2.set_xlabel('Network Depth (# Hidden Layers)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('PR-AUC (Validation)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Impact of Network Depth', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Width vs Performance\n",
    "ax3 = axes[0, 2]\n",
    "ax3.scatter(results_df['width'], results_df['pr_auc'], s=200, alpha=0.7, color='coral', edgecolor='black', linewidth=2)\n",
    "for idx, row in results_df.iterrows():\n",
    "    ax3.annotate(row['architecture'], (row['width'], row['pr_auc']), \n",
    "                fontsize=8, ha='center', va='bottom')\n",
    "ax3.set_xlabel('Maximum Layer Width', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('PR-AUC (Validation)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Impact of Network Width', fontsize=13, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Training Time vs Performance\n",
    "ax4 = axes[1, 0]\n",
    "ax4.scatter(results_df['train_time'], results_df['pr_auc'], s=200, alpha=0.7, color='green', edgecolor='black', linewidth=2)\n",
    "for idx, row in results_df.iterrows():\n",
    "    ax4.annotate(row['architecture'], (row['train_time'], row['pr_auc']), \n",
    "                fontsize=8, ha='right', va='bottom')\n",
    "ax4.set_xlabel('Training Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('PR-AUC (Validation)', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Training Efficiency', fontsize=13, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. PR-AUC Ranking\n",
    "ax5 = axes[1, 1]\n",
    "sorted_results = results_df.sort_values('pr_auc')\n",
    "colors = ['#e74c3c' if row['architecture'] == best_arch['architecture'] else '#3498db' \n",
    "         for _, row in sorted_results.iterrows()]\n",
    "ax5.barh(range(len(sorted_results)), sorted_results['pr_auc'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax5.set_yticks(range(len(sorted_results)))\n",
    "ax5.set_yticklabels(sorted_results['architecture'], fontsize=10)\n",
    "ax5.set_xlabel('PR-AUC (Validation)', fontsize=12, fontweight='bold')\n",
    "ax5.set_title('Architecture Ranking by PR-AUC', fontsize=13, fontweight='bold')\n",
    "ax5.axvline(results_df['pr_auc'].mean(), color='black', linestyle='--', linewidth=2, label='Mean')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 6. F1 Score Ranking\n",
    "ax6 = axes[1, 2]\n",
    "sorted_f1 = results_df.sort_values('fraud_f1')\n",
    "ax6.barh(range(len(sorted_f1)), sorted_f1['fraud_f1'], color='purple', alpha=0.7, edgecolor='black')\n",
    "ax6.set_yticks(range(len(sorted_f1)))\n",
    "ax6.set_yticklabels(sorted_f1['architecture'], fontsize=10)\n",
    "ax6.set_xlabel('F1 Score (Fraud Class)', fontsize=12, fontweight='bold')\n",
    "ax6.set_title('Architecture Ranking by F1 Score', fontsize=13, fontweight='bold')\n",
    "ax6.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle('Neural Network Architecture Comparison - Fraud Detection', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/architecture_comparison_comprehensive.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Comprehensive visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800593f0",
   "metadata": {},
   "source": [
    "## 7. Learning Curves for Top 3 Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edcdd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for top 3 architectures\n",
    "top_3_archs = results_df.head(3)['architecture'].values\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "for idx, arch_name in enumerate(top_3_archs):\n",
    "    history = histories[arch_name]\n",
    "    plot_learning_curves(history, title=f'{arch_name.upper()}', ax=axes[idx])\n",
    "\n",
    "plt.suptitle('Learning Curves - Top 3 Architectures by PR-AUC', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/top3_learning_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Learning curves saved for top 3 architectures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a943e",
   "metadata": {},
   "source": [
    "## 8. Comparison with Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77dfa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline results\n",
    "baseline_path = os.path.join(config.RESULTS_DIR, 'tables', 'baseline_performance_targets.csv')\n",
    "baselines = pd.read_csv(baseline_path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEURAL NETWORKS vs BASELINES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nBaseline Performance (from Notebook 02):\")\n",
    "print(baselines[['model', 'pr_auc', 'f1_fraud']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nBest Neural Network ({best_arch['architecture']}):\")\n",
    "print(f\"  PR-AUC:    {best_arch['pr_auc']:.4f}\")\n",
    "print(f\"  F1 (Fraud): {best_arch['fraud_f1']:.4f}\")\n",
    "\n",
    "# Calculate improvement\n",
    "best_baseline_pr_auc = baselines['pr_auc'].max()\n",
    "improvement = (best_arch['pr_auc'] - best_baseline_pr_auc) / best_baseline_pr_auc * 100\n",
    "\n",
    "print(f\"\\nğŸ¯ Improvement over Best Baseline:\")\n",
    "print(f\"  PR-AUC: +{improvement:.2f}%\")\n",
    "print(f\"  Absolute gain: +{best_arch['pr_auc'] - best_baseline_pr_auc:.4f}\")\n",
    "\n",
    "if improvement > 15:\n",
    "    print(\"\\nâœ… Goal Achieved: >15% improvement over baselines!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Target not yet met. Need {15 - improvement:.2f}% more improvement.\")\n",
    "    print(\"   â†’ Try regularization experiments in Notebook 04\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff863b4",
   "metadata": {},
   "source": [
    "## 9. Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d54cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" NOTEBOOK 03 SUMMARY - ARCHITECTURE EXPLORATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"âœ“ Architectures tested: {len(architectures_to_test)}\")\n",
    "print(f\"âœ“ Best architecture: {best_arch['architecture']} - {best_arch['layers']}\")\n",
    "print(f\"âœ“ Best PR-AUC: {best_arch['pr_auc']:.4f}\")\n",
    "print(f\"âœ“ Improvement over baselines: +{improvement:.2f}%\")\n",
    "\n",
    "print(\"\\nğŸ“Š Key Findings:\")\n",
    "print(\"  1. Depth Analysis:\")\n",
    "depth_corr = results_df[['depth', 'pr_auc']].corr().iloc[0, 1]\n",
    "print(f\"     - Depth-Performance correlation: {depth_corr:.3f}\")\n",
    "if depth_corr > 0:\n",
    "    print(\"     - Deeper networks show performance improvement\")\n",
    "else:\n",
    "    print(\"     - Diminishing returns from excessive depth\")\n",
    "\n",
    "print(\"  2. Width Analysis:\")\n",
    "width_corr = results_df[['width', 'pr_auc']].corr().iloc[0, 1]\n",
    "print(f\"     - Width-Performance correlation: {width_corr:.3f}\")\n",
    "\n",
    "print(\"  3. Complexity vs Performance:\")\n",
    "param_corr = results_df[['num_params', 'pr_auc']].corr().iloc[0, 1]\n",
    "print(f\"     - Parameters-Performance correlation: {param_corr:.3f}\")\n",
    "if param_corr > 0.5:\n",
    "    print(\"     - Larger models perform better (no overfitting detected)\")\n",
    "else:\n",
    "    print(\"     - Optimal model size achieved, avoid over-parameterization\")\n",
    "\n",
    "print(\"\\nğŸ“ Outputs:\")\n",
    "outputs = [\n",
    "    results_path,\n",
    "    '../results/figures/architecture_comparison_comprehensive.png',\n",
    "    '../results/figures/top3_learning_curves.png',\n",
    "    f'{config.MODELS_DIR}/arch_*.keras (8 models saved)'\n",
    "]\n",
    "for output in outputs:\n",
    "    print(f\"   {output}\")\n",
    "\n",
    "print(\"\\nâœ… Notebook 03 Complete!\")\n",
    "print(\"ğŸš€ Next: Notebook 04 - Regularization Experiments\")\n",
    "print(\"   Goal: Apply dropout, L2, and batch normalization to best architecture\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
