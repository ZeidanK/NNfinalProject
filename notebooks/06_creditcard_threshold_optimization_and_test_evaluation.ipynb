{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326dd59c",
   "metadata": {},
   "source": [
    "# Notebook 06: creditcard Threshold Optimization & Test Evaluation\n",
    "\n",
    "**Dataset**: creditcard.csv (Real-World ULB)\n",
    "\n",
    "**Objective**: Final threshold tuning on validation set and ONE-TIME test evaluation\n",
    "\n",
    "**Primary Metric**: PR-AUC (optimal for extreme imbalance)\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL CONSTRAINT**: Test set evaluation performed EXACTLY ONCE to prevent overfitting\n",
    "\n",
    "**Workflow**:\n",
    "1. Load best NN model from Notebook 05 (by validation PR-AUC)\n",
    "2. Threshold optimization on validation set\n",
    "3. **Final test set evaluation** (single evaluation)\n",
    "4. Error analysis (FP/FN patterns)\n",
    "5. Business cost simulation\n",
    "6. Final model recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "# Import and reload modules\n",
    "import config\n",
    "import src.nn_architectures\n",
    "import src.nn_training_utils\n",
    "import src.evaluation_metrics\n",
    "import src.visualization_utils\n",
    "\n",
    "importlib.reload(config)\n",
    "importlib.reload(src.nn_architectures)\n",
    "importlib.reload(src.nn_training_utils)\n",
    "importlib.reload(src.evaluation_metrics)\n",
    "importlib.reload(src.visualization_utils)\n",
    "\n",
    "from src.evaluation_metrics import compute_fraud_metrics, print_classification_summary\n",
    "from src.visualization_utils import (\n",
    "    plot_confusion_matrix,\n",
    "    plot_precision_recall_curve\n",
    ")\n",
    "\n",
    "# Set random seeds\n",
    "config.set_random_seeds()\n",
    "\n",
    "# Get dataset config\n",
    "ds_config = config.get_dataset_config('creditcard')\n",
    "\n",
    "print(\"‚úì Imports complete\")\n",
    "print(f\"‚úì TensorFlow version: {tf.__version__}\")\n",
    "print(f\"‚úì Random seed set to {config.RANDOM_SEED}\")\n",
    "print(f\"‚úì Dataset: creditcard.csv (Real-World ULB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3b5c9",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e603788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original data\n",
    "df = pd.read_csv(ds_config['data_path'])\n",
    "X = df[ds_config['feature_cols']].values\n",
    "y = df[ds_config['target_col']].values\n",
    "\n",
    "# Load saved split indices\n",
    "train_idx = np.load(ds_config['train_idx'])\n",
    "val_idx = np.load(ds_config['val_idx'])\n",
    "test_idx = np.load(ds_config['test_idx'])\n",
    "\n",
    "# Split data using saved indices\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_val, y_val = X[val_idx], y[val_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "# Load fitted scaler\n",
    "scaler = joblib.load(ds_config['scaler_path'])\n",
    "\n",
    "# Transform data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úì Data loaded and preprocessed:\")\n",
    "print(f\"  Train: {X_train_scaled.shape[0]:,} samples\")\n",
    "print(f\"  Val:   {X_val_scaled.shape[0]:,} samples\")\n",
    "print(f\"  Test:  {X_test_scaled.shape[0]:,} samples\")\n",
    "print(f\"  Features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"\\n‚úì Class distribution:\")\n",
    "print(f\"  Train fraud rate: {y_train.mean()*100:.4f}%\")\n",
    "print(f\"  Val fraud rate:   {y_val.mean()*100:.4f}%\")\n",
    "print(f\"  Test fraud rate:  {y_test.mean()*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef21755",
   "metadata": {},
   "source": [
    "## 2. Load Best Model from Notebook 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e380c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment logs from Notebook 05\n",
    "nn_experiments_log = ds_config['experiment_logs_dir'] / 'nn_experiments.csv'\n",
    "\n",
    "if nn_experiments_log.exists():\n",
    "    experiments_df = pd.read_csv(nn_experiments_log)\n",
    "    \n",
    "    # Filter for creditcard experiments\n",
    "    creditcard_experiments = experiments_df[experiments_df['dataset_name'] == 'creditcard']\n",
    "    \n",
    "    # Sort by validation PR-AUC\n",
    "    creditcard_experiments = creditcard_experiments.sort_values('pr_auc', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" TOP 5 MODELS BY VALIDATION PR-AUC\")\n",
    "    print(\"=\"*80)\n",
    "    print(creditcard_experiments[['experiment_id', 'experiment_type', 'pr_auc', 'f1_fraud', 'recall_fraud']].head().to_string(index=False))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get best experiment\n",
    "    best_exp = creditcard_experiments.iloc[0]\n",
    "    best_exp_id = best_exp['experiment_id']\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Model: {best_exp_id}\")\n",
    "    print(f\"   Type: {best_exp['experiment_type']}\")\n",
    "    print(f\"   Validation PR-AUC: {best_exp['pr_auc']:.4f}\")\n",
    "    print(f\"   Validation F1 (Fraud): {best_exp['f1_fraud']:.4f}\")\n",
    "    print(f\"   Validation Recall (Fraud): {best_exp['recall_fraud']:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No experiment logs found. Please run Notebook 05 first.\")\n",
    "    best_exp_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "if best_exp_id:\n",
    "    # Try different possible model paths\n",
    "    possible_paths = [\n",
    "        ds_config['models_dir'] / 'neural_networks' / f'{best_exp_id}.keras',\n",
    "        ds_config['models_dir'] / 'neural_networks' / f'{best_exp_id}_{best_exp[\"model_name\"]}.keras',\n",
    "    ]\n",
    "    \n",
    "    best_model = None\n",
    "    for model_path in possible_paths:\n",
    "        if model_path.exists():\n",
    "            best_model = keras.models.load_model(model_path)\n",
    "            print(f\"\\n‚úì Model loaded from: {model_path}\")\n",
    "            break\n",
    "    \n",
    "    if best_model is None:\n",
    "        print(f\"\\n‚ö†Ô∏è Model file not found. Searched paths:\")\n",
    "        for p in possible_paths:\n",
    "            print(f\"   - {p}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No best model identified. Please run Notebook 05 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629fecce",
   "metadata": {},
   "source": [
    "## 3. Threshold Optimization on Validation Set\n",
    "\n",
    "Test multiple thresholds to find optimal balance between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f26b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation predictions (probabilities)\n",
    "y_val_pred_proba = best_model.predict(X_val_scaled, verbose=0).flatten()\n",
    "\n",
    "# Test threshold candidates\n",
    "threshold_results = []\n",
    "\n",
    "for threshold in config.THRESHOLD_CANDIDATES:\n",
    "    y_val_pred = (y_val_pred_proba >= threshold).astype(int)\n",
    "    metrics = compute_fraud_metrics(y_val, y_val_pred, y_val_pred_proba)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'threshold': threshold,\n",
    "        'precision_fraud': metrics['precision_fraud'],\n",
    "        'recall_fraud': metrics['recall_fraud'],\n",
    "        'f1_fraud': metrics['f1_fraud'],\n",
    "        'pr_auc': metrics['pr_auc']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\" THRESHOLD OPTIMIZATION RESULTS (Validation Set)\")\n",
    "print(\"=\"*90)\n",
    "print(threshold_df.to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Find best threshold by F1\n",
    "best_threshold_idx = threshold_df['f1_fraud'].idxmax()\n",
    "best_threshold = threshold_df.loc[best_threshold_idx, 'threshold']\n",
    "best_f1 = threshold_df.loc[best_threshold_idx, 'f1_fraud']\n",
    "\n",
    "print(f\"\\nüéØ Optimal Threshold: {best_threshold}\")\n",
    "print(f\"   F1 (Fraud): {best_f1:.4f}\")\n",
    "print(f\"   Precision (Fraud): {threshold_df.loc[best_threshold_idx, 'precision_fraud']:.4f}\")\n",
    "print(f\"   Recall (Fraud): {threshold_df.loc[best_threshold_idx, 'recall_fraud']:.4f}\")\n",
    "\n",
    "# Save threshold results\n",
    "threshold_path = ds_config['tables_dir'] / 'threshold_optimization.csv'\n",
    "threshold_df.to_csv(threshold_path, index=False)\n",
    "print(f\"\\n‚úì Threshold results saved to: {threshold_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3ed3e",
   "metadata": {},
   "source": [
    "## 4. Visualize Precision-Recall Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0a5759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot threshold optimization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Precision vs Recall\n",
    "axes[0].plot(threshold_df['recall_fraud'], threshold_df['precision_fraud'], \n",
    "             marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[0].scatter(threshold_df.loc[best_threshold_idx, 'recall_fraud'],\n",
    "                threshold_df.loc[best_threshold_idx, 'precision_fraud'],\n",
    "                color='red', s=200, marker='*', zorder=5, label=f'Optimal (t={best_threshold})')\n",
    "axes[0].set_xlabel('Recall (Fraud)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Precision (Fraud)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Precision-Recall Tradeoff', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend(fontsize=10)\n",
    "\n",
    "# Plot 2: F1 vs Threshold\n",
    "axes[1].plot(threshold_df['threshold'], threshold_df['f1_fraud'], \n",
    "             marker='o', linewidth=2, markersize=8, color='green')\n",
    "axes[1].axvline(best_threshold, color='red', linestyle='--', linewidth=2, label=f'Optimal (t={best_threshold})')\n",
    "axes[1].set_xlabel('Threshold', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('F1 Score (Fraud)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('F1 Score vs Threshold', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = ds_config['figures_dir'] / 'threshold_optimization.png'\n",
    "plt.savefig(fig_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\n",
    "print(f\"‚úì Figure saved to: {fig_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9ca704",
   "metadata": {},
   "source": [
    "## 5. ‚ö†Ô∏è FINAL TEST SET EVALUATION (ONE-TIME ONLY)\n",
    "\n",
    "**CRITICAL**: This is the ONLY evaluation on the test set. No iteration allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f26d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ‚ö†Ô∏è  FINAL TEST SET EVALUATION - ONE-TIME ONLY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nEvaluating on held-out test set with optimal threshold...\\n\")\n",
    "\n",
    "# Get test predictions\n",
    "y_test_pred_proba = best_model.predict(X_test_scaled, verbose=0).flatten()\n",
    "y_test_pred = (y_test_pred_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Compute test metrics\n",
    "test_metrics = compute_fraud_metrics(y_test, y_test_pred, y_test_pred_proba)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" FINAL TEST SET PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModel: {best_exp_id}\")\n",
    "print(f\"Threshold: {best_threshold}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  PR-AUC:             {test_metrics['pr_auc']:.4f}\")\n",
    "print(f\"  ROC-AUC:            {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  F1 (Fraud):         {test_metrics['f1_fraud']:.4f}\")\n",
    "print(f\"  Recall (Fraud):     {test_metrics['recall_fraud']:.4f}\")\n",
    "print(f\"  Precision (Fraud):  {test_metrics['precision_fraud']:.4f}\")\n",
    "print(f\"  Accuracy:           {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Negatives:  {test_metrics['tn']:,}\")\n",
    "print(f\"  False Positives: {test_metrics['fp']:,}\")\n",
    "print(f\"  False Negatives: {test_metrics['fn']:,}\")\n",
    "print(f\"  True Positives:  {test_metrics['tp']:,}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save test results\n",
    "test_results = {\n",
    "    'model_id': best_exp_id,\n",
    "    'threshold': best_threshold,\n",
    "    'test_size': len(y_test),\n",
    "    'fraud_count': int(y_test.sum()),\n",
    "    **test_metrics\n",
    "}\n",
    "\n",
    "test_results_df = pd.DataFrame([test_results])\n",
    "test_results_path = ds_config['tables_dir'] / 'final_test_evaluation.csv'\n",
    "test_results_df.to_csv(test_results_path, index=False)\n",
    "print(f\"\\n‚úì Test results saved to: {test_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e5539",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "fig_path = ds_config['figures_dir'] / 'final_test_confusion_matrix.png'\n",
    "plot_confusion_matrix(\n",
    "    y_test,\n",
    "    y_test_pred,\n",
    "    labels=['Legitimate', 'Fraud'],\n",
    "    save_path=str(fig_path),\n",
    "    title=f'Test Set Confusion Matrix (Threshold={best_threshold})'\n",
    ")\n",
    "print(f\"‚úì Confusion matrix saved to: {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2815dbb",
   "metadata": {},
   "source": [
    "## 7. Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f72fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PR curve\n",
    "fig_path = ds_config['figures_dir'] / 'final_test_pr_curve.png'\n",
    "plot_precision_recall_curve(\n",
    "    y_test,\n",
    "    y_test_pred_proba,\n",
    "    save_path=str(fig_path),\n",
    "    title='Test Set Precision-Recall Curve'\n",
    ")\n",
    "print(f\"‚úì PR curve saved to: {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3114d25",
   "metadata": {},
   "source": [
    "## 8. Error Analysis\n",
    "\n",
    "Analyze false positives and false negatives to understand model failure modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify error cases\n",
    "false_positives = (y_test == 0) & (y_test_pred == 1)\n",
    "false_negatives = (y_test == 1) & (y_test_pred == 0)\n",
    "true_positives = (y_test == 1) & (y_test_pred == 1)\n",
    "true_negatives = (y_test == 0) & (y_test_pred == 0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFalse Positives: {false_positives.sum():,} ({false_positives.sum()/len(y_test)*100:.2f}%)\")\n",
    "print(f\"False Negatives: {false_negatives.sum():,} ({false_negatives.sum()/len(y_test)*100:.2f}%)\")\n",
    "print(f\"True Positives:  {true_positives.sum():,} ({true_positives.sum()/len(y_test)*100:.2f}%)\")\n",
    "print(f\"True Negatives:  {true_negatives.sum():,} ({true_negatives.sum()/len(y_test)*100:.2f}%)\")\n",
    "\n",
    "# Analyze prediction confidence for errors\n",
    "print(f\"\\nüìä Prediction Confidence Analysis:\")\n",
    "print(f\"\\nFalse Positives (predicted fraud incorrectly):\")\n",
    "print(f\"  Mean confidence: {y_test_pred_proba[false_positives].mean():.4f}\")\n",
    "print(f\"  Median confidence: {np.median(y_test_pred_proba[false_positives]):.4f}\")\n",
    "print(f\"  Min confidence: {y_test_pred_proba[false_positives].min():.4f}\")\n",
    "print(f\"  Max confidence: {y_test_pred_proba[false_positives].max():.4f}\")\n",
    "\n",
    "print(f\"\\nFalse Negatives (missed fraud):\")\n",
    "print(f\"  Mean confidence: {y_test_pred_proba[false_negatives].mean():.4f}\")\n",
    "print(f\"  Median confidence: {np.median(y_test_pred_proba[false_negatives]):.4f}\")\n",
    "print(f\"  Min confidence: {y_test_pred_proba[false_negatives].min():.4f}\")\n",
    "print(f\"  Max confidence: {y_test_pred_proba[false_negatives].max():.4f}\")\n",
    "\n",
    "print(f\"\\nTrue Positives (correctly detected fraud):\")\n",
    "print(f\"  Mean confidence: {y_test_pred_proba[true_positives].mean():.4f}\")\n",
    "print(f\"  Median confidence: {np.median(y_test_pred_proba[true_positives]):.4f}\")\n",
    "print(f\"  Min confidence: {y_test_pred_proba[true_positives].min():.4f}\")\n",
    "print(f\"  Max confidence: {y_test_pred_proba[true_positives].max():.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f5f5ad",
   "metadata": {},
   "source": [
    "## 9. Business Cost Analysis\n",
    "\n",
    "Simulate business costs assuming:\n",
    "- False Positive cost: Manual review ($25 per transaction)\n",
    "- False Negative cost: Fraud loss ($100 per transaction)\n",
    "- True Positive benefit: Fraud prevented ($100 per transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cost parameters\n",
    "COST_FP = 25    # Manual review cost\n",
    "COST_FN = 100   # Fraud loss\n",
    "BENEFIT_TP = 100  # Fraud prevented\n",
    "\n",
    "# Calculate costs\n",
    "total_fp_cost = test_metrics['fp'] * COST_FP\n",
    "total_fn_cost = test_metrics['fn'] * COST_FN\n",
    "total_tp_benefit = test_metrics['tp'] * BENEFIT_TP\n",
    "\n",
    "net_benefit = total_tp_benefit - total_fp_cost - total_fn_cost\n",
    "\n",
    "# Calculate cost if we did nothing (all fraud passes through)\n",
    "do_nothing_cost = y_test.sum() * COST_FN\n",
    "\n",
    "# Calculate savings\n",
    "savings = do_nothing_cost - total_fn_cost - total_fp_cost\n",
    "savings_rate = (savings / do_nothing_cost) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" BUSINESS COST ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nCost Assumptions:\")\n",
    "print(f\"  False Positive: ${COST_FP} (manual review)\")\n",
    "print(f\"  False Negative: ${COST_FN} (fraud loss)\")\n",
    "print(f\"  True Positive:  ${BENEFIT_TP} (fraud prevented)\")\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  False Positives: {test_metrics['fp']:,} ‚Üí Cost: ${total_fp_cost:,.2f}\")\n",
    "print(f\"  False Negatives: {test_metrics['fn']:,} ‚Üí Cost: ${total_fn_cost:,.2f}\")\n",
    "print(f\"  True Positives:  {test_metrics['tp']:,} ‚Üí Benefit: ${total_tp_benefit:,.2f}\")\n",
    "\n",
    "print(f\"\\nNet Analysis:\")\n",
    "print(f\"  Total Cost:         ${(total_fp_cost + total_fn_cost):,.2f}\")\n",
    "print(f\"  Total Benefit:      ${total_tp_benefit:,.2f}\")\n",
    "print(f\"  Net Benefit:        ${net_benefit:,.2f}\")\n",
    "\n",
    "print(f\"\\nComparison to Baseline (No Model):\")\n",
    "print(f\"  Do-Nothing Cost:    ${do_nothing_cost:,.2f}\")\n",
    "print(f\"  Model Cost:         ${(total_fp_cost + total_fn_cost):,.2f}\")\n",
    "print(f\"  Savings:            ${savings:,.2f} ({savings_rate:.1f}% reduction)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save business analysis\n",
    "business_analysis = {\n",
    "    'model_id': best_exp_id,\n",
    "    'threshold': best_threshold,\n",
    "    'cost_fp': COST_FP,\n",
    "    'cost_fn': COST_FN,\n",
    "    'benefit_tp': BENEFIT_TP,\n",
    "    'total_fp_cost': total_fp_cost,\n",
    "    'total_fn_cost': total_fn_cost,\n",
    "    'total_tp_benefit': total_tp_benefit,\n",
    "    'net_benefit': net_benefit,\n",
    "    'do_nothing_cost': do_nothing_cost,\n",
    "    'savings': savings,\n",
    "    'savings_rate_percent': savings_rate\n",
    "}\n",
    "\n",
    "business_df = pd.DataFrame([business_analysis])\n",
    "business_path = ds_config['tables_dir'] / 'business_cost_analysis.csv'\n",
    "business_df.to_csv(business_path, index=False)\n",
    "print(f\"\\n‚úì Business analysis saved to: {business_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4376848",
   "metadata": {},
   "source": [
    "## 10. Summary & Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" NOTEBOOK 06 SUMMARY - FINAL TEST EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_exp_id}\")\n",
    "print(f\"   Optimal Threshold: {best_threshold}\")\n",
    "\n",
    "print(f\"\\nüìä Test Set Performance:\")\n",
    "print(f\"   PR-AUC:        {test_metrics['pr_auc']:.4f}\")\n",
    "print(f\"   ROC-AUC:       {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"   F1 (Fraud):    {test_metrics['f1_fraud']:.4f}\")\n",
    "print(f\"   Recall (Fraud): {test_metrics['recall_fraud']:.4f}\")\n",
    "print(f\"   Precision (Fraud): {test_metrics['precision_fraud']:.4f}\")\n",
    "\n",
    "print(f\"\\nüí∞ Business Impact:\")\n",
    "print(f\"   Cost Savings:  ${savings:,.2f} ({savings_rate:.1f}% reduction)\")\n",
    "print(f\"   Net Benefit:   ${net_benefit:,.2f}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Error Analysis:\")\n",
    "print(f\"   False Positives: {test_metrics['fp']:,} (over-flagging legitimate)\")\n",
    "print(f\"   False Negatives: {test_metrics['fn']:,} (missed fraud)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Model Readiness:\")\n",
    "if test_metrics['pr_auc'] >= 0.75 and test_metrics['recall_fraud'] >= 0.70:\n",
    "    print(\"   Status: READY FOR PRODUCTION\")\n",
    "    print(\"   Recommendation: Deploy with real-time monitoring\")\n",
    "elif test_metrics['pr_auc'] >= 0.65 and test_metrics['recall_fraud'] >= 0.60:\n",
    "    print(\"   Status: ACCEPTABLE WITH MONITORING\")\n",
    "    print(\"   Recommendation: Deploy with enhanced fraud analyst support\")\n",
    "else:\n",
    "    print(\"   Status: NEEDS IMPROVEMENT\")\n",
    "    print(\"   Recommendation: Further tuning or additional features required\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(\"   1. Cross-dataset analysis (Notebook 07)\")\n",
    "print(\"   2. Compare generalization across datasets\")\n",
    "print(\"   3. Identify transferable design principles\")\n",
    "print(\"   4. Document final findings and limitations\")\n",
    "\n",
    "print(f\"\\nüìÅ Artifacts Created:\")\n",
    "print(f\"   - Threshold optimization: {ds_config['tables_dir']}/threshold_optimization.csv\")\n",
    "print(f\"   - Test evaluation: {ds_config['tables_dir']}/final_test_evaluation.csv\")\n",
    "print(f\"   - Business analysis: {ds_config['tables_dir']}/business_cost_analysis.csv\")\n",
    "print(f\"   - Confusion matrix: {ds_config['figures_dir']}/final_test_confusion_matrix.png\")\n",
    "print(f\"   - PR curve: {ds_config['figures_dir']}/final_test_pr_curve.png\")\n",
    "\n",
    "print(\"\\n‚úÖ Notebook 06 Complete!\")\n",
    "print(\"üéØ Ready for Notebook 07: Cross-dataset analysis and conclusions\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
