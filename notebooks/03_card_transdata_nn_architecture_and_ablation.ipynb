{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72624694",
   "metadata": {},
   "source": [
    "# Notebook 03: card_transdata Neural Network Architecture & Ablation\n",
    "\n",
    "**Dataset**: card_transdata.csv (Synthetic)\n",
    "\n",
    "**Objective**: Explore 8 neural network architectures (ARCH-01 to ARCH-08) and conduct ablation studies (ABL-01 to ABL-05) to identify best design.\n",
    "\n",
    "**Primary Metric**: PR-AUC (optimal for class imbalance)\n",
    "\n",
    "**Critical Constraint**: Neural networks are NOT expected to outperform Random Forest on this synthetic dataset. The value lies in:\n",
    "- Clean architecture experimentation (no RF dominance frustration)\n",
    "- Identifying transferable design principles\n",
    "- Understanding regularization/overfitting patterns\n",
    "- Selecting best architecture for creditcard.csv validation\n",
    "\n",
    "**Experiments**:\n",
    "1. **Architecture Search** (ARCH-01 to ARCH-08): Test 8 architectures from shallow to deep\n",
    "2. **Ablation Study** (ABL-01 to ABL-05): Isolate impact of Dropout, L2, BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978bf22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports complete\n",
      "‚úì TensorFlow version: 2.15.0\n",
      "‚úì Random seed set to 42\n",
      "‚úì Dataset: card_transdata.csv (Synthetic)\n",
      "‚úì Config keys: ['data_path', 'results_dir', 'models_dir', 'figures_dir', 'tables_dir', 'logs_dir', 'experiment_logs_dir', 'train_idx', 'val_idx', 'test_idx', 'scaler_path', 'target_col', 'feature_cols', 'epochs', 'early_stop_patience', 'rf_config']\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "# Import and reload modules to ensure we get the latest code\n",
    "import config\n",
    "import src.nn_architectures\n",
    "import src.nn_training_utils\n",
    "import src.evaluation_metrics\n",
    "import src.visualization_utils\n",
    "\n",
    "# Reload ALL modules to pick up any recent changes\n",
    "importlib.reload(config)\n",
    "importlib.reload(src.nn_architectures)\n",
    "importlib.reload(src.nn_training_utils)\n",
    "importlib.reload(src.evaluation_metrics)\n",
    "importlib.reload(src.visualization_utils)\n",
    "\n",
    "from src.nn_architectures import build_fraud_detection_nn\n",
    "from src.nn_training_utils import train_nn_with_early_stopping, log_experiment\n",
    "from src.evaluation_metrics import compute_fraud_metrics, print_classification_summary\n",
    "from src.visualization_utils import (\n",
    "    plot_training_history,\n",
    "    plot_confusion_matrix,\n",
    "    plot_precision_recall_curve\n",
    ")\n",
    "\n",
    "# Set random seeds\n",
    "config.set_random_seeds()\n",
    "\n",
    "# Get dataset config (AFTER reloading config)\n",
    "ds_config = config.get_dataset_config('card_transdata')\n",
    "\n",
    "print(\"‚úì Imports complete\")\n",
    "print(f\"‚úì TensorFlow version: {tf.__version__}\")\n",
    "print(f\"‚úì Random seed set to {config.RANDOM_SEED}\")\n",
    "print(f\"‚úì Dataset: card_transdata.csv (Synthetic)\")\n",
    "print(f\"‚úì Config keys: {list(ds_config.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d061765",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "453c1345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data loaded and preprocessed:\n",
      "  Train: 700,000 samples\n",
      "  Val:   150,000 samples\n",
      "  Test:  150,000 samples\n",
      "  Features: 7\n",
      "\n",
      "‚úì Class distribution:\n",
      "  Train fraud rate: 8.7403%\n",
      "  Val fraud rate:   8.7407%\n",
      "  Test fraud rate:  8.7400%\n"
     ]
    }
   ],
   "source": [
    "# Load original data\n",
    "df = pd.read_csv(ds_config['data_path'])\n",
    "X = df[ds_config['feature_cols']].values\n",
    "y = df[ds_config['target_col']].values\n",
    "\n",
    "# Load saved split indices\n",
    "train_idx = np.load(ds_config['train_idx'])\n",
    "val_idx = np.load(ds_config['val_idx'])\n",
    "test_idx = np.load(ds_config['test_idx'])\n",
    "\n",
    "# Split data using saved indices\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_val, y_val = X[val_idx], y[val_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "# Load fitted scaler\n",
    "scaler = joblib.load(ds_config['scaler_path'])\n",
    "\n",
    "# Transform data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úì Data loaded and preprocessed:\")\n",
    "print(f\"  Train: {X_train_scaled.shape[0]:,} samples\")\n",
    "print(f\"  Val:   {X_val_scaled.shape[0]:,} samples\")\n",
    "print(f\"  Test:  {X_test_scaled.shape[0]:,} samples\")\n",
    "print(f\"  Features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"\\n‚úì Class distribution:\")\n",
    "print(f\"  Train fraud rate: {y_train.mean()*100:.4f}%\")\n",
    "print(f\"  Val fraud rate:   {y_val.mean()*100:.4f}%\")\n",
    "print(f\"  Test fraud rate:  {y_test.mean()*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d3e180",
   "metadata": {},
   "source": [
    "## 2. Load Baseline Performance for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b10a0acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Baseline Performance Targets (Validation Set):\n",
      "============================================================\n",
      "              model   pr_auc  roc_auc  f1_fraud  precision_fraud  recall_fraud  accuracy\n",
      "Logistic Regression 0.757077  0.97972  0.717315         0.576369      0.949508  0.934587\n",
      "      Random Forest 1.000000  1.00000  0.999847         1.000000      0.999695  0.999973\n",
      "============================================================\n",
      "\n",
      "üéØ Random Forest PR-AUC: 1.0000\n",
      "\n",
      "‚ö†Ô∏è  Note: NNs are NOT expected to beat this on synthetic data\n",
      "    Goal: Find best NN architecture for transfer to creditcard.csv\n"
     ]
    }
   ],
   "source": [
    "# Load baseline results from Notebook 02\n",
    "baseline_results = pd.read_csv(ds_config['tables_dir'] / 'baseline_results.csv')\n",
    "\n",
    "print(\"üìä Baseline Performance Targets (Validation Set):\")\n",
    "print(\"=\"*60)\n",
    "print(baseline_results.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract RF performance as target\n",
    "rf_pr_auc = baseline_results[baseline_results['model'] == 'Random Forest']['pr_auc'].values[0]\n",
    "print(f\"\\nüéØ Random Forest PR-AUC: {rf_pr_auc:.4f}\")\n",
    "print(f\"\\n‚ö†Ô∏è  Note: NNs are NOT expected to beat this on synthetic data\")\n",
    "print(f\"    Goal: Find best NN architecture for transfer to creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005ad46a",
   "metadata": {},
   "source": [
    "## 3. Architecture Search (ARCH-01 to ARCH-08)\n",
    "\n",
    "Test 8 architectures from shallow to deep with fixed regularization (Dropout=0.3, L2=0.001, BatchNorm=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "154acc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " ARCHITECTURE SEARCH: Testing 8 Architectures\n",
      "======================================================================\n",
      "Fixed hyperparameters: Dropout=0.3, L2=0.001, BatchNorm=True\n",
      "Optimization: class_weight='balanced', EarlyStopping(patience=10)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize experiment log\n",
    "architecture_results = []\n",
    "\n",
    "# Create results directory\n",
    "(ds_config['models_dir'] / 'neural_networks').mkdir(parents=True, exist_ok=True)\n",
    "(ds_config['experiment_logs_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" ARCHITECTURE SEARCH: Testing 8 Architectures\")\n",
    "print(\"=\"*70)\n",
    "print(\"Fixed hyperparameters: Dropout=0.3, L2=0.001, BatchNorm=True\")\n",
    "print(\"Optimization: class_weight='balanced', EarlyStopping(patience=10)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d9b576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training ARCH-01: shallow_tiny\n",
      "Hidden layers: [32]\n",
      "======================================================================\n",
      "WARNING:tensorflow:From C:\\Users\\GOD\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'compiled'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      9\u001b[39m model = build_fraud_detection_nn(\n\u001b[32m     10\u001b[39m     input_dim=X_train_scaled.shape[\u001b[32m1\u001b[39m],\n\u001b[32m     11\u001b[39m     hidden_layers=arch_config[\u001b[33m'\u001b[39m\u001b[33mlayers\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     use_batch_norm=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Train with early stopping\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m history = \u001b[43mtrain_nn_with_early_stopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEARLY_STOPPING_PATIENCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbalanced\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     28\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[32m     31\u001b[39m y_val_pred_proba = model.predict(X_val_scaled, verbose=\u001b[32m0\u001b[39m).flatten()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\GOD\\Documents\\NNfinalProject\\src\\nn_training_utils.py:334\u001b[39m, in \u001b[36mtrain_nn_with_early_stopping\u001b[39m\u001b[34m(model, X_train, y_train, X_val, y_val, epochs, batch_size, patience, class_weight, learning_rate, verbose)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[33;03mTrain neural network with early stopping and standard configuration.\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    331\u001b[39m \u001b[33;03m    Training history object\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# Compile model if not already compiled\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompiled\u001b[49m:\n\u001b[32m    335\u001b[39m     model = compile_model(model, learning_rate=learning_rate)\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# Get class weights\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Sequential' object has no attribute 'compiled'"
     ]
    }
   ],
   "source": [
    "# Iterate through architecture configurations\n",
    "for arch_id, arch_config in config.ARCHITECTURES_TO_TEST.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {arch_id}: {arch_config['name']}\")\n",
    "    print(f\"Hidden layers: {arch_config['layers']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_fraud_detection_nn(\n",
    "        input_dim=X_train_scaled.shape[1],\n",
    "        hidden_layers=arch_config['layers'],\n",
    "        dropout_rate=0.3,\n",
    "        l2_reg=0.001,\n",
    "        use_batch_norm=True\n",
    "    )\n",
    "    \n",
    "    # Train with early stopping\n",
    "    history = train_nn_with_early_stopping(\n",
    "        model=model,\n",
    "        X_train=X_train_scaled,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val_scaled,\n",
    "        y_val=y_val,\n",
    "        epochs=config.MAX_EPOCHS,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        patience=config.EARLY_STOPPING_PATIENCE,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    y_val_pred_proba = model.predict(X_val_scaled, verbose=0).flatten()\n",
    "    y_val_pred = (y_val_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = compute_fraud_metrics(y_val, y_val_pred, y_val_pred_proba)\n",
    "    \n",
    "    print(f\"\\nüìä {arch_id} Validation Performance:\")\n",
    "    print(f\"  PR-AUC:        {metrics['pr_auc']:.4f}\")\n",
    "    print(f\"  ROC-AUC:       {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"  F1 (Fraud):    {metrics['f1_fraud']:.4f}\")\n",
    "    print(f\"  Recall (Fraud): {metrics['recall_fraud']:.4f}\")\n",
    "    print(f\"  Precision (Fraud): {metrics['precision_fraud']:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = ds_config['models_dir'] / 'neural_networks' / f'{arch_id}_{arch_config[\"name\"]}.keras'\n",
    "    model.save(model_path)\n",
    "    print(f\"\\n‚úì Model saved to: {model_path}\")\n",
    "    \n",
    "    # Plot learning curves\n",
    "    fig_path = ds_config['figures_dir'] / 'nn_architectures' / f'{arch_id}_learning_curves.png'\n",
    "    plot_training_history(\n",
    "        history,\n",
    "        save_path=str(fig_path),\n",
    "        title=f'{arch_id}: {arch_config[\"name\"]} - Learning Curves'\n",
    "    )\n",
    "    \n",
    "    # Log experiment\n",
    "    log_experiment(\n",
    "        experiment_id=arch_id,\n",
    "        dataset_name='card_transdata',\n",
    "        experiment_type='architecture',\n",
    "        model_name=arch_config['name'],\n",
    "        architecture=arch_config['layers'],\n",
    "        hyperparameters={\n",
    "            'dropout': 0.3,\n",
    "            'l2': 0.001,\n",
    "            'batch_norm': True,\n",
    "            'batch_size': config.BATCH_SIZE,\n",
    "            'epochs_trained': len(history.history['loss'])\n",
    "        },\n",
    "        metrics=metrics,\n",
    "        log_path=ds_config['experiment_logs_dir'] / 'nn_experiments.csv'\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    architecture_results.append({\n",
    "        'experiment_id': arch_id,\n",
    "        'architecture': arch_config['name'],\n",
    "        'layers': str(arch_config['layers']),\n",
    "        'pr_auc': metrics['pr_auc'],\n",
    "        'roc_auc': metrics['roc_auc'],\n",
    "        'f1_fraud': metrics['f1_fraud'],\n",
    "        'recall_fraud': metrics['recall_fraud'],\n",
    "        'precision_fraud': metrics['precision_fraud']\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì Architecture Search Complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06016c7",
   "metadata": {},
   "source": [
    "## 4. Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "arch_df = pd.DataFrame(architecture_results)\n",
    "arch_df = arch_df.sort_values('pr_auc', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ARCHITECTURE RANKING BY PR-AUC (Validation Set)\")\n",
    "print(\"=\"*80)\n",
    "print(arch_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify best architecture\n",
    "best_arch = arch_df.iloc[0]\n",
    "print(f\"\\nüèÜ Best Architecture: {best_arch['experiment_id']} ({best_arch['architecture']})\")\n",
    "print(f\"   Layers: {best_arch['layers']}\")\n",
    "print(f\"   PR-AUC: {best_arch['pr_auc']:.4f}\")\n",
    "print(f\"   F1 (Fraud): {best_arch['f1_fraud']:.4f}\")\n",
    "\n",
    "# Save ranking\n",
    "arch_ranking_path = ds_config['tables_dir'] / 'architecture_ranking.csv'\n",
    "arch_df.to_csv(arch_ranking_path, index=False)\n",
    "print(f\"\\n‚úì Architecture ranking saved to: {arch_ranking_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69637419",
   "metadata": {},
   "source": [
    "## 5. Ablation Study (ABL-01 to ABL-05)\n",
    "\n",
    "Use best architecture and systematically remove regularization components to isolate their impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4fc454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best architecture layers\n",
    "best_arch_id = best_arch['experiment_id']\n",
    "best_arch_layers = config.ARCHITECTURES_TO_TEST[best_arch_id]['layers']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" ABLATION STUDY: Isolating Regularization Impact\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Using best architecture: {best_arch_id} ({best_arch['architecture']})\")\n",
    "print(f\"Layers: {best_arch_layers}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ablation_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f05e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through ablation configurations\n",
    "for abl_id, abl_config in config.ABLATION_EXPERIMENTS.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {abl_id}: {abl_config['description']}\")\n",
    "    print(f\"Dropout={abl_config['dropout']}, L2={abl_config['l2']}, BatchNorm={abl_config['batch_norm']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Build model with ablation config\n",
    "    model = build_fraud_detection_nn(\n",
    "        input_dim=X_train_scaled.shape[1],\n",
    "        hidden_layers=best_arch_layers,\n",
    "        dropout_rate=abl_config['dropout'],\n",
    "        l2_reg=abl_config['l2'],\n",
    "        use_batch_norm=abl_config['batch_norm']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = train_nn_with_early_stopping(\n",
    "        model=model,\n",
    "        X_train=X_train_scaled,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val_scaled,\n",
    "        y_val=y_val,\n",
    "        epochs=config.MAX_EPOCHS,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        patience=config.EARLY_STOPPING_PATIENCE,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    y_val_pred_proba = model.predict(X_val_scaled, verbose=0).flatten()\n",
    "    y_val_pred = (y_val_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = compute_fraud_metrics(y_val, y_val_pred, y_val_pred_proba)\n",
    "    \n",
    "    print(f\"\\nüìä {abl_id} Validation Performance:\")\n",
    "    print(f\"  PR-AUC:        {metrics['pr_auc']:.4f}\")\n",
    "    print(f\"  F1 (Fraud):    {metrics['f1_fraud']:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = ds_config['models_dir'] / 'neural_networks' / f'{abl_id}_ablation.keras'\n",
    "    model.save(model_path)\n",
    "    \n",
    "    # Plot learning curves\n",
    "    fig_path = ds_config['figures_dir'] / 'ablation_studies' / f'{abl_id}_learning_curves.png'\n",
    "    plot_training_history(\n",
    "        history,\n",
    "        save_path=str(fig_path),\n",
    "        title=f'{abl_id}: {abl_config[\"description\"]} - Learning Curves'\n",
    "    )\n",
    "    \n",
    "    # Log experiment\n",
    "    log_experiment(\n",
    "        experiment_id=abl_id,\n",
    "        dataset_name='card_transdata',\n",
    "        experiment_type='ablation',\n",
    "        model_name=f\"{best_arch['architecture']}_ablation\",\n",
    "        architecture=best_arch_layers,\n",
    "        hyperparameters={\n",
    "            'dropout': abl_config['dropout'],\n",
    "            'l2': abl_config['l2'],\n",
    "            'batch_norm': abl_config['batch_norm'],\n",
    "            'batch_size': config.BATCH_SIZE,\n",
    "            'epochs_trained': len(history.history['loss'])\n",
    "        },\n",
    "        metrics=metrics,\n",
    "        log_path=ds_config['experiment_logs_dir'] / 'nn_experiments.csv'\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    ablation_results.append({\n",
    "        'experiment_id': abl_id,\n",
    "        'description': abl_config['description'],\n",
    "        'dropout': abl_config['dropout'],\n",
    "        'l2': abl_config['l2'],\n",
    "        'batch_norm': abl_config['batch_norm'],\n",
    "        'pr_auc': metrics['pr_auc'],\n",
    "        'f1_fraud': metrics['f1_fraud']\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì Ablation Study Complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858d123",
   "metadata": {},
   "source": [
    "## 6. Ablation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f031223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "abl_df = pd.DataFrame(ablation_results)\n",
    "abl_df = abl_df.sort_values('pr_auc', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\" ABLATION STUDY RESULTS (Validation Set)\")\n",
    "print(\"=\"*90)\n",
    "print(abl_df.to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Save ablation results\n",
    "abl_path = ds_config['tables_dir'] / 'ablation_results.csv'\n",
    "abl_df.to_csv(abl_path, index=False)\n",
    "print(f\"\\n‚úì Ablation results saved to: {abl_path}\")\n",
    "\n",
    "# Compare with full regularization\n",
    "full_reg = abl_df[abl_df['experiment_id'] == 'ABL-05'].iloc[0]\n",
    "no_reg = abl_df[abl_df['experiment_id'] == 'ABL-01'].iloc[0]\n",
    "\n",
    "print(f\"\\nüìä Regularization Impact:\")\n",
    "print(f\"  No regularization (ABL-01): PR-AUC = {no_reg['pr_auc']:.4f}\")\n",
    "print(f\"  Full regularization (ABL-05): PR-AUC = {full_reg['pr_auc']:.4f}\")\n",
    "print(f\"  Improvement: {(full_reg['pr_auc'] - no_reg['pr_auc'])*100:.2f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53588d",
   "metadata": {},
   "source": [
    "## 7. Summary & Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f03925",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" NOTEBOOK 03 SUMMARY - card_transdata NN ARCHITECTURE & ABLATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüèÜ Best Architecture: {best_arch['experiment_id']} - {best_arch['architecture']}\")\n",
    "print(f\"   Layers: {best_arch['layers']}\")\n",
    "print(f\"   Validation PR-AUC: {best_arch['pr_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä Baseline Comparison:\")\n",
    "print(f\"   Random Forest: {rf_pr_auc:.4f}\")\n",
    "print(f\"   Best NN:       {best_arch['pr_auc']:.4f}\")\n",
    "print(f\"   Difference:    {(best_arch['pr_auc'] - rf_pr_auc)*100:+.2f} percentage points\")\n",
    "\n",
    "if best_arch['pr_auc'] < rf_pr_auc:\n",
    "    print(\"\\n‚úÖ Expected Result: RF outperforms NN on synthetic data\")\n",
    "    print(\"   This does NOT invalidate the NN experiments:\")\n",
    "    print(\"   - Clean architecture exploration completed\")\n",
    "    print(\"   - Ablation insights gained\")\n",
    "    print(\"   - Design principles transferable to creditcard.csv\")\n",
    "\n",
    "print(f\"\\nüî¨ Ablation Insights:\")\n",
    "print(f\"   No regularization:   PR-AUC = {no_reg['pr_auc']:.4f}\")\n",
    "print(f\"   Full regularization: PR-AUC = {full_reg['pr_auc']:.4f}\")\n",
    "print(f\"   Regularization value: {(full_reg['pr_auc'] - no_reg['pr_auc'])*100:+.2f}pp\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   1. Transfer best architecture to creditcard.csv (Notebook 04-06)\")\n",
    "print(f\"   2. Validate on real-world ULB dataset\")\n",
    "print(f\"   3. Test regularization strategies (REG-01 to REG-08)\")\n",
    "print(f\"   4. Compare NN vs. RF on production-quality data\")\n",
    "\n",
    "print(\"\\nüìÅ Artifacts Created:\")\n",
    "print(f\"   - {len(architecture_results)} architecture models\")\n",
    "print(f\"   - {len(ablation_results)} ablation models\")\n",
    "print(f\"   - Experiment logs: {ds_config['experiment_logs_dir']}/nn_experiments.csv\")\n",
    "print(f\"   - Learning curves: {ds_config['figures_dir']}/\")\n",
    "print(f\"   - Rankings: {ds_config['tables_dir']}/\")\n",
    "\n",
    "print(\"\\n‚úÖ Notebook 03 Complete!\")\n",
    "print(\"üéØ Ready for Notebook 04: creditcard preprocessing & baselines\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
