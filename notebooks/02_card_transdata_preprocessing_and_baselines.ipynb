{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f79cba1",
   "metadata": {},
   "source": [
    "# Notebook 02: card_transdata Preprocessing & Baselines\n",
    "\n",
    "**Dataset**: card_transdata.csv (Synthetic - ~1M transactions, 7 features, ~0.8% fraud)\n",
    "\n",
    "**Objective**: Establish preprocessing pipeline and baseline performance for the synthetic dataset.\n",
    "\n",
    "**Critical for Reproducibility**: This notebook creates:\n",
    "- Train/validation/test splits (70/15/15, stratified)\n",
    "- Fitted StandardScaler (on training data only)\n",
    "- Baseline models (Logistic Regression, Random Forest)\n",
    "\n",
    "**Expected Baseline Performance**: Random Forest may achieve PR-AUC ‚âà 1.0 (near-perfect) due to high feature separability. This is **acceptable** - the synthetic dataset's role is enabling clean architecture exploration.\n",
    "\n",
    "**Contents**:\n",
    "1. Load card_transdata.csv\n",
    "2. Create stratified splits and save indices\n",
    "3. Fit and save StandardScaler\n",
    "4. Train baseline models (LR, RF)\n",
    "5. Save baseline performance for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97070cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import config\n",
    "from src.evaluation_metrics import compute_fraud_metrics, print_classification_summary\n",
    "from src.visualization_utils import plot_confusion_matrix, plot_precision_recall_curve\n",
    "\n",
    "# Set random seeds\n",
    "config.set_random_seeds()\n",
    "\n",
    "# Ensure directories\n",
    "config.ensure_directories()\n",
    "\n",
    "# Get dataset config\n",
    "ds_config = config.get_dataset_config('card_transdata')\n",
    "\n",
    "print(\"‚úì Imports complete\")\n",
    "print(f\"‚úì Random seed set to {config.RANDOM_SEED}\")\n",
    "print(f\"‚úì Dataset: card_transdata.csv (Synthetic)\")\n",
    "print(f\"‚úì Results directory: {ds_config['results_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7935e4",
   "metadata": {},
   "source": [
    "## 1. Load card_transdata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d30e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(ds_config['data_path'])\n",
    "print(f\"‚úì Loaded {df.shape[0]:,} transactions with {df.shape[1]} features\")\n",
    "print(f\"  Path: {ds_config['data_path']}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df[ds_config['feature_cols']].values\n",
    "y = df[ds_config['target_col']].values\n",
    "\n",
    "print(f\"\\n‚úì Features shape: {X.shape}\")\n",
    "print(f\"‚úì Target shape: {y.shape}\")\n",
    "print(f\"‚úì Fraud prevalence: {y.mean()*100:.4f}%\")\n",
    "print(f\"‚úì Imbalance ratio: 1:{(1-y.mean())/y.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f76ce47",
   "metadata": {},
   "source": [
    "## 2. Create Stratified Train/Validation/Test Splits\n",
    "\n",
    "**Critical**: Split indices are saved to ensure all subsequent notebooks use identical partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e0df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: 70% train, 30% temp (for val+test)\n",
    "X_train, X_temp, y_train, y_temp, train_idx, temp_idx = train_test_split(\n",
    "    X, y, np.arange(len(y)),\n",
    "    test_size=0.30,\n",
    "    stratify=y,\n",
    "    random_state=config.RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Second split: 50% of temp (15% of total) for validation, 50% for test\n",
    "X_val, X_test, y_val, y_test, val_idx_temp, test_idx_temp = train_test_split(\n",
    "    X_temp, y_temp, np.arange(len(y_temp)),\n",
    "    test_size=0.50,\n",
    "    stratify=y_temp,\n",
    "    random_state=config.RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Map temp indices back to original indices\n",
    "val_idx = temp_idx[val_idx_temp]\n",
    "test_idx = temp_idx[test_idx_temp]\n",
    "\n",
    "# Save indices for reproducibility\n",
    "np.save(ds_config['train_idx'], train_idx)\n",
    "np.save(ds_config['val_idx'], val_idx)\n",
    "np.save(ds_config['test_idx'], test_idx)\n",
    "\n",
    "print(\"‚úì Data split complete (70/15/15):\")\n",
    "print(f\"  Train: {len(train_idx):,} samples ({len(train_idx)/len(y)*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_idx):,} samples ({len(val_idx)/len(y)*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_idx):,} samples ({len(test_idx)/len(y)*100:.1f}%)\")\n",
    "print(f\"\\n‚úì Class distribution:\")\n",
    "print(f\"  Train fraud rate: {y_train.mean()*100:.4f}%\")\n",
    "print(f\"  Val fraud rate:   {y_val.mean()*100:.4f}%\")\n",
    "print(f\"  Test fraud rate:  {y_test.mean()*100:.4f}%\")\n",
    "print(f\"\\n‚úì Split indices saved to:\")\n",
    "print(f\"  {ds_config['train_idx']}\")\n",
    "print(f\"  {ds_config['val_idx']}\")\n",
    "print(f\"  {ds_config['test_idx']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f00ad",
   "metadata": {},
   "source": [
    "## 3. Fit StandardScaler on Training Data\n",
    "\n",
    "**Data Leakage Prevention**: Scaler fitted ONLY on training data, then applied to val/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit scaler on training data ONLY\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform validation and test using fitted scaler\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save fitted scaler\n",
    "joblib.dump(scaler, ds_config['scaler_path'])\n",
    "\n",
    "print(\"‚úì StandardScaler fitted on training data\")\n",
    "print(f\"‚úì Scaler saved to: {ds_config['scaler_path']}\")\n",
    "print(f\"\\n‚úì Feature statistics (from training data):\")\n",
    "print(f\"  Mean range: [{scaler.mean_.min():.4f}, {scaler.mean_.max():.4f}]\")\n",
    "print(f\"  Std range:  [{scaler.scale_.min():.4f}, {scaler.scale_.max():.4f}]\")\n",
    "print(f\"\\n‚úì Scaled data statistics:\")\n",
    "print(f\"  Train - Mean: {X_train_scaled.mean():.6f}, Std: {X_train_scaled.std():.6f}\")\n",
    "print(f\"  Val   - Mean: {X_val_scaled.mean():.6f}, Std: {X_val_scaled.std():.6f}\")\n",
    "print(f\"  Test  - Mean: {X_test_scaled.mean():.6f}, Std: {X_test_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb76f3",
   "metadata": {},
   "source": [
    "## 4. Baseline Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"Training Logistic Regression with balanced class weights...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lr_model = LogisticRegression(**config.LR_CONFIG)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predictions on validation set\n",
    "y_val_pred_proba_lr = lr_model.predict_proba(X_val_scaled)[:, 1]\n",
    "y_val_pred_lr = lr_model.predict(X_val_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "lr_metrics = compute_fraud_metrics(y_val, y_val_pred_lr, y_val_pred_proba_lr)\n",
    "\n",
    "print(f\"\\n‚úì Training complete in {train_time:.2f} seconds\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOGISTIC REGRESSION - Validation Performance\")\n",
    "print(\"=\"*60)\n",
    "print_classification_summary(y_val, y_val_pred_lr, y_val_pred_proba_lr, \"Validation Set\")\n",
    "\n",
    "# Save model\n",
    "lr_model_path = ds_config['models_dir'] / 'baselines' / 'lr_baseline.pkl'\n",
    "joblib.dump(lr_model, lr_model_path)\n",
    "print(f\"\\n‚úì Model saved to: {lr_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3c644",
   "metadata": {},
   "source": [
    "## 5. Baseline Model 2: Random Forest\n",
    "\n",
    "**Expected**: RF may achieve PR-AUC ‚âà 1.0 on this synthetic dataset (high separability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dde66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "print(\"Training Random Forest with balanced class weights...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_model = RandomForestClassifier(**ds_config['rf_config'])\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predictions on validation set\n",
    "y_val_pred_proba_rf = rf_model.predict_proba(X_val_scaled)[:, 1]\n",
    "y_val_pred_rf = rf_model.predict(X_val_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "rf_metrics = compute_fraud_metrics(y_val, y_val_pred_rf, y_val_pred_proba_rf)\n",
    "\n",
    "print(f\"\\n‚úì Training complete in {train_time:.2f} seconds\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANDOM FOREST - Validation Performance\")\n",
    "print(\"=\"*60)\n",
    "print_classification_summary(y_val, y_val_pred_rf, y_val_pred_proba_rf, \"Validation Set\")\n",
    "\n",
    "# Save model\n",
    "rf_model_path = ds_config['models_dir'] / 'baselines' / 'rf_baseline.pkl'\n",
    "joblib.dump(rf_model, rf_model_path)\n",
    "print(f\"\\n‚úì Model saved to: {rf_model_path}\")\n",
    "\n",
    "if rf_metrics['pr_auc'] > 0.95:\n",
    "    print(\"\\n‚ö†Ô∏è  Note: Random Forest achieves very high performance (expected on synthetic data)\")\n",
    "    print(\"    This does NOT invalidate neural network experiments - NNs enable:\")\n",
    "    print(\"    - Clean architecture ablation studies\")\n",
    "    print(\"    - Transferable design principles to real-world data\")\n",
    "    print(\"    - Controlled overfitting analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9bd99e",
   "metadata": {},
   "source": [
    "## 6. Visualize Baseline Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6390b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize baseline comparisons\n",
    "print(\"Generating baseline comparison visualizations...\")\n",
    "\n",
    "# Confusion matrices\n",
    "plot_confusion_matrix(y_val, y_val_pred_lr, ['Legitimate', 'Fraud'], \n",
    "                     save_path=str(ds_config['figures_dir'] / 'lr_confusion_matrix.png'),\n",
    "                     title='Logistic Regression - Confusion Matrix')\n",
    "\n",
    "plot_confusion_matrix(y_val, y_val_pred_rf, ['Legitimate', 'Fraud'],\n",
    "                     save_path=str(ds_config['figures_dir'] / 'rf_confusion_matrix.png'),\n",
    "                     title='Random Forest - Confusion Matrix')\n",
    "\n",
    "# PR curves\n",
    "plot_precision_recall_curve(y_val, y_val_pred_proba_lr,\n",
    "                            save_path=str(ds_config['figures_dir'] / 'lr_pr_curve.png'),\n",
    "                            title='Logistic Regression - PR Curve')\n",
    "\n",
    "plot_precision_recall_curve(y_val, y_val_pred_proba_rf,\n",
    "                            save_path=str(ds_config['figures_dir'] / 'rf_pr_curve.png'),\n",
    "                            title='Random Forest - PR Curve')\n",
    "\n",
    "print(\"\\n‚úì All baseline visualizations saved to:\")\n",
    "print(f\"  {ds_config['figures_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b145bd7",
   "metadata": {},
   "source": [
    "## 7. Save Baseline Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b86042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline summary DataFrame\n",
    "baseline_results = pd.DataFrame({\n",
    "    'model': ['Logistic Regression', 'Random Forest'],\n",
    "    'pr_auc': [lr_metrics['pr_auc'], rf_metrics['pr_auc']],\n",
    "    'roc_auc': [lr_metrics['roc_auc'], rf_metrics['roc_auc']],\n",
    "    'f1_fraud': [lr_metrics['f1_fraud'], rf_metrics['f1_fraud']],\n",
    "    'precision_fraud': [lr_metrics['precision_fraud'], rf_metrics['precision_fraud']],\n",
    "    'recall_fraud': [lr_metrics['recall_fraud'], rf_metrics['recall_fraud']],\n",
    "    'accuracy': [lr_metrics['accuracy'], rf_metrics['accuracy']]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "baseline_path = ds_config['tables_dir'] / 'baseline_results.csv'\n",
    "baseline_results.to_csv(baseline_path, index=False)\n",
    "\n",
    "print(\"‚úì Baseline performance summary:\")\n",
    "print(baseline_results.to_string(index=False))\n",
    "print(f\"\\n‚úì Saved to: {baseline_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad0a018",
   "metadata": {},
   "source": [
    "## 8. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f42b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" NOTEBOOK 02 SUMMARY - card_transdata PREPROCESSING & BASELINES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚úì Dataset: card_transdata.csv (Synthetic)\")\n",
    "print(f\"‚úì Data split: {len(train_idx):,} train / {len(val_idx):,} val / {len(test_idx):,} test\")\n",
    "print(f\"‚úì Scaler fitted and saved\")\n",
    "print(f\"‚úì Split indices saved for reproducibility\")\n",
    "print(\"\\nüìä Baseline Performance (Validation Set):\")\n",
    "print(f\"  Logistic Regression:\")\n",
    "print(f\"    - PR-AUC: {lr_metrics['pr_auc']:.4f}\")\n",
    "print(f\"    - F1 (Fraud): {lr_metrics['f1_fraud']:.4f}\")\n",
    "print(f\"  Random Forest:\")\n",
    "print(f\"    - PR-AUC: {rf_metrics['pr_auc']:.4f}\")\n",
    "print(f\"    - F1 (Fraud): {rf_metrics['f1_fraud']:.4f}\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"  - Neural networks are NOT expected to beat RF on this synthetic dataset\")\n",
    "print(\"  - Value lies in: architecture exploration, ablation studies, design principles\")\n",
    "print(\"  - Best architecture will be transferred to creditcard.csv for validation\")\n",
    "\n",
    "print(\"\\nüìÅ Artifacts Created:\")\n",
    "print(f\"  - Split indices: {ds_config['results_dir']}/*.npy\")\n",
    "print(f\"  - Fitted scaler: {ds_config['scaler_path']}\")\n",
    "print(f\"  - Baseline models: {ds_config['models_dir']}/baselines/\")\n",
    "print(f\"  - Baseline results: {baseline_path}\")\n",
    "print(f\"  - Visualizations: {ds_config['figures_dir']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Notebook 02 Complete!\")\n",
    "print(\"üöÄ Ready for Notebook 03: card_transdata NN Architecture & Ablation\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
